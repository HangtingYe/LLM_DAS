# Anomalies that are the most difficult for a LOF detector to identify are those that do not conform to the typical pattern of an outlier, which is an isolated point with a significantly lower local density than its neighbors.

# My policy for generating such "hard" anomalies is to create **anomalous micro-clusters**. These are small, dense groups of points located far away from the normal data distribution. Here's why this approach specifically targets the weakness of LOF:

# 1.  **Exploiting the "Local" Nature of LOF**: LOF determines the anomaly score of a point by comparing its local density to that of its immediate neighbors. When a point is part of a dense micro-cluster, its nearest neighbors are the other points within that same cluster.
# 2.  **Achieving a Low Anomaly Score**: Since all points in the micro-cluster are close to each other, their local densities are high and, more importantly, very similar. According to Step 5 of the LOF algorithm, the LOF score is the ratio of the neighbors' densities to the point's own density. For a point in a micro-cluster, this ratio will be very close to 1.0, which is the score for a perfect inlier.
# 3.  **Global vs. Local Anomaly**: While the micro-cluster as a whole is a clear "global" anomaly (being far from the training data), each individual point within it appears "locally" normal. This discrepancy makes them hard for LOF to detect, yet they are valuable for training a more robust system that can learn to identify these patterns.

# The generation process involves three key steps:
# 1.  **Identify "Borderline" Seeds**: We find normal points that already lie on the low-density fringes of the training data distribution. These are identified as the training points with the highest anomaly scores.
# 2.  **Determine a New Location**: We use these borderline points as a starting base. To ensure the new anomaly is in an empty region, we push the point outwards, away from the center of the main data cloud. This becomes the center of our new micro-cluster.
# 3.  **Generate a Dense Micro-Cluster**: We generate the final anomaly points by adding very small, controlled noise around the new cluster center. The "tightness" (density) of this cluster is strategically determined by observing the density of the *most normal* (core) points in the original training data, ensuring our new cluster appears as dense or denser.
import numpy as np
def generate_hard_anomalies(n_samples: int, model, X_train: np.ndarray):
    """
    Generates anomalies that are difficult for a Local Outlier Factor (LOF) detector to identify.

    This function implements a "micro-cluster" generation policy, which is specifically designed
    to challenge the LOF algorithm. LOF identifies outliers based on low local density compared to
    neighbors. This policy creates small, dense clusters of points in sparse areas of the feature
    space. Each point within such a micro-cluster has a high local density relative to its new
    neighbors (the other points in the same cluster), resulting in a low LOF score (~1.0) that
    makes it appear normal, even though the cluster itself is far from the training data.

    The generation process is as follows:
    1.  Find 'borderline' normal samples from X_train. These are points with the highest anomaly
        scores among the normal data, likely residing on the edges of dense regions. They serve
        as ideal starting "seeds" for our anomalies.
    2.  Estimate the density of the "core" normal data by calculating the average nearest-neighbor
        distance for the points with the lowest anomaly scores. This is used to determine how
        "tight" our new micro-cluster needs to be to fool the LOF detector.
    3.  For each anomaly to be generated, a seed point is chosen and moved into an empty region
        by pushing it away from the centroid of the training data. This new location becomes the
        center of the micro-cluster.
    4.  The final anomaly is generated by adding a small amount of noise to this new cluster center,
        creating a point within a dense, isolated group.

    Args:
        n_samples (int): The number of hard anomaly samples to generate.
        model: A trained LOF-style detector that has a `predict_score()` method.
               The method should return a score where a higher value indicates a higher
               likelihood of being an anomaly.
        X_train (np.ndarray): The training data, containing only normal samples, with shape
                              (n_train_samples, n_features).

    Returns:
        np.ndarray: An array of generated hard anomalies of shape (n_samples, n_features).
    """
    # All necessary imports are placed inside the function as required.
    import numpy as np
    from sklearn.neighbors import NearestNeighbors

    # Ensure the training data is a numpy array.
    X_train = np.asarray(X_train)
    if X_train.ndim != 2:
        raise ValueError("X_train must be a 2D array.")
    
    n_train, n_features = X_train.shape

    if n_train == 0:
        raise ValueError("X_train cannot be empty.")

    # --- ANOMALY GENERATION POLICY IMPLEMENTATION ---

    # STEP 1: Identify 'borderline' normal samples to use as seeds.
    # We calculate the anomaly scores for all training points. The points with the highest scores
    # are the most "outlier-like" of the normal set, making them perfect seeds.
    train_scores = model.predict_score(X_train)

    # Sort indices by score to find the most anomalous points in the training set.
    # This works whether high scores are positive (e.g., 1.5) or negative (e.g., -10),
    # as argsort finds the indices from smallest to largest score. We take from the end.
    # We select a pool of up to 20 borderline points to add variety to the generated samples.
    n_seeds = min(20, n_train)
    borderline_indices = np.argsort(train_scores)[-n_seeds:]
    X_borderline = X_train[borderline_indices]

    # STEP 2: Estimate the "tightness" required for the new micro-cluster.
    # To appear dense, the points in our new cluster must be closer to each other than
    # points in the dense regions of the original data. We estimate this by looking
    # at the most normal ("core") points from the training set.
    n_core = min(50, n_train)
    core_indices = np.argsort(train_scores)[:n_core]
    X_core = X_train[core_indices]
    
    # Calculate the average distance to the nearest neighbor for these core points.
    # This gives us a quantitative measure of "density" in the normal data.
    if len(X_core) > 1:
        # Use k=2 because k=1 is the point itself (distance is 0).
        nn = NearestNeighbors(n_neighbors=2).fit(X_core)
        distances, _ = nn.kneighbors(X_core)
        avg_core_dist = np.mean(distances[:, 1])
    else:
        # Fallback for very small datasets: use a fraction of the overall data's std deviation.
        avg_core_dist = np.mean(np.std(X_train, axis=0)) * 0.1

    # The standard deviation for the noise we add will be a small fraction of this core distance
    # to ensure our new micro-cluster is exceptionally dense. A value of 0.1 is chosen to
    # create a very tight cluster.
    cluster_tightness_std = avg_core_dist * 0.1 + 1e-9 # Add epsilon for numerical stability

    # STEP 3: Generate the hard anomaly samples.
    # Each anomaly is a point in a new micro-cluster located far from the main data.
    generated_anomalies = []
    data_centroid = np.mean(X_train, axis=0)

    for i in range(n_samples):
        # a. Select a seed point from our borderline set (cycle through them).
        seed_point = X_borderline[i % len(X_borderline)]
        
        # b. Define a direction pointing away from the data's center of mass.
        direction_vector = seed_point - data_centroid
        norm = np.linalg.norm(direction_vector)
        if norm > 0:
            direction_vector /= norm
        else:
            # If a seed is at the centroid, pick a random direction.
            direction_vector = np.random.randn(n_features)
            direction_vector /= np.linalg.norm(direction_vector)

        # c. Create the center of the new micro-cluster.
        # We move the seed point along the "outward" direction. The step size is scaled by
        # `avg_core_dist` to ensure it lands in a meaningfully sparse region relative to the data's density.
        # A scaling factor of 5.0 makes the shift significant.
        step_size = 5.0 * avg_core_dist
        cluster_center = seed_point + direction_vector * step_size
        
        # d. Generate the final anomaly by adding tight Gaussian noise to the cluster center.
        # This places the point within the conceptual dense micro-cluster.
        noise = np.random.normal(loc=0.0, scale=cluster_tightness_std, size=n_features)
        hard_anomaly = cluster_center + noise
        
        generated_anomalies.append(hard_anomaly)
        
    return np.array(generated_anomalies)